# IndependentStudy

Evaluated complex datasets and embeddings for Neural Transformers Architecture with Large Language Models with advanced bioinformatics Python libraries like bioctfl and used multiple scoring metrics like AUC, ROC, and MCC

Spearheaded background research pertaining to Large Language Models and Neural Transformers Architecture and found the most efficient language model for carrying out the gene therapy and drug discovery applications of bioinformatics

Formulated original research publication with holistic analysis of state-of-art frameworks for Large Language Models with 50% higher optimized ASO design than the original findings

Used LLaMA2, GPT-3.5-Turbo, and Galactica-6.7B after evaluating holistically other LLM models like LlaSMol, T0pp, and Mistral-7B for zero-shot and few-shot prompting to improve R squared and RMSE scores

Future Scope: 

Add Chain of Thought Prompting and other models like T0pp, Mistral-7B, and LlaSMol to improve the reasoning capabilities of ASO efficacy binding and additionally add more GPT models to get a holistic evaluation of RMSE and R squared scores

Add more few-shot parameters for k instead of just 3 - maybe 8, 11, 15, and 25 to get more accurate few-shot results
